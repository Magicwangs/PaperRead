{"./":{"url":"./","title":"Introduction","keywords":"","body":"IntroductionIntroduction ███████╗██╗ ██╗ ██████╗ ██╗ ██╗ ███╗ ███╗███████╗ ██╔════╝██║ ██║██╔═══██╗██║ ██║ ████╗ ████║██╔════╝ ███████╗███████║██║ ██║██║ █╗ ██║ ██╔████╔██║█████╗ ╚════██║██╔══██║██║ ██║██║███╗██║ ██║╚██╔╝██║██╔══╝ ███████║██║ ██║╚██████╔╝╚███╔███╔╝ ██║ ╚═╝ ██║███████╗ ╚══════╝╚═╝ ╚═╝ ╚═════╝ ╚══╝╚══╝ ╚═╝ ╚═╝╚══════╝ 因为写博客太麻烦, 所以改用gitbook记录平时看的一些paper或是好的书. No Translation, Only Keypoint. By MagicWang，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2018-10-30 17:00:49 "},"BASENET/":{"url":"BASENET/","title":"BASENET","keywords":"","body":"BASENETBASENET ATTENTION ResNet Residual Attention Network for Image Classification By MagicWang，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2018-10-30 17:00:49 "},"BASENET/Residual_Attention_Network_for_Image_Classification.html":{"url":"BASENET/Residual_Attention_Network_for_Image_Classification.html","title":"Residual Attention Network for Image Classification","keywords":"","body":"Residual Attention Network for Image Classification Authors: Fei Wang, Mengqing Jiang Link: https://arxiv.org/abs/1704.06904 Tags: SenseTime Attention Year: 2017 Official Code: https://github.com/fwang91/residual-attention-network Motivation Attention机制不仅仅可以帮助channel的加强, 还可以增强物体在不同位置的表达 KeyWord The Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask.However, repeated dot product will lead to severe degradation of both useful and useless information in this process. The attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast. Bottom-up top-down feedforward attention 使用类似于Stacked Hourglass 沙漏型的结构 在feature map上增加一个soft weight Bottem-up 结构产生了低分辨率但是强语义信息的Feature Map Top-Down 结构则 负责产生 高分辨率的Dense的结果 Skip-Connect 结构来帮助信息的融合 Trunk Branch and Mask Branch Trunk主干分支: PreAct 的残差块(Resnet V2) -- T(x) Mask分支: bottom-up top-down 结构 --和 T(x) 相同Size的 M(x) 最终 H(x)=M(x)∗T(x)H(x) = M(x)*T(x)H(x)=M(x)∗T(x) 直接点乘 WHY 如果只是简单的堆叠Attention模块,会导致性能的下降 因为 mask的值(经过sigmoid)是 0-1, 反复的乘以这个mask 会导致 feature map的值越来越小 soft mask 可能会破坏Trunk分支的特征 所以最终 H(x)=(1+M(x))∗F(x)H(x) = (1+M(x)) * F(x)H(x)=(1+M(x))∗F(x) 其中 0M(x)100M(x)1 希望 mask能够抑制主干分支的噪声, 增强重要的特征 Soft Mask Branch 使用 max-pool来降低增大感受野,双线性插值来增大分辨率 bottom-up和top-down对称 Mask分支的主要目的还是增强 Trunk分支 Spatial Attention and Channel Attention 只需要通过一个 sigmoid 就能实现最佳的效果 不需要 SENet那样的 仅对通道 weight或是仅对空间 weight By MagicWang，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2018-10-30 17:00:49 "},"DETECTION/":{"url":"DETECTION/","title":"DETECTION","keywords":"","body":"DETECTIONFirst Of AllDETECTION First Of All mean Average precision(mAP): mAP定义及相关概念mAP: mean Average Precision, 即各类别AP的平均值 AP: PR曲线下面积，后文会详细讲解 PR曲线: Precision-Recall曲线 Precision: TP / (TP + FP) Recall: TP / (TP + FN) TP: IoU>{0,0.1,...,1}的检测框数量（同一Ground Truth只计算一次） FP: IoUFN: 没有检测到的GT的数量 R-CNN Object Detection Cascade R-CNN: Delving into High Quality Object Detection By MagicWang，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2018-10-30 20:15:18 "},"DETECTION/Cascade_R_CNN.html":{"url":"DETECTION/Cascade_R_CNN.html","title":"Cascade R-CNN: Delving into High Quality Object Detection","keywords":"","body":"Cascade R-CNN: Delving into High Quality Object Detection Authors: Zhaowei Cai, Nuno Vasconcelos Link: https://arxiv.org/abs/1712.00726 Tags: R-CNN Object Detection Year: 2017 Official Code: https://github.com/zhaoweicai/cascade-rcnn Motivation 在目标检测时, 需要分类和回归, 分类的时候选择IoU阈值来确定正负样本, 只有正样本会参与到回归中 但是 随着 IoU阈值的提高 正样本 成指数级的减少 inference阶段, 固定阈值 会出现很多 close false postives KeyWord a single detector can only be optimal for a single quality level. Detail 图1: WHY By MagicWang，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2018-10-30 20:15:26 "},"3D/":{"url":"3D/","title":"3D","keywords":"","body":"3D3D By MagicWang，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2018-10-30 17:00:49 "},"MORE/":{"url":"MORE/","title":"MORE","keywords":"","body":"MOREMORE BatchNorm 我知道的关于BN的一切 GitBook AboutGitBook By MagicWang，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2018-10-30 17:00:49 "},"MORE/More_Batch_Norm.html":{"url":"MORE/More_Batch_Norm.html","title":"Batch Norm","keywords":"","body":"我知道的关于BN的一切 Authors: Magic Tags: BatchNorm Year: Alive Summary WHAT HOW DETAIL need read By MagicWang，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2018-10-30 17:00:49 "},"MORE/AboutGitBook.html":{"url":"MORE/AboutGitBook.html","title":"About GitBook","keywords":"","body":"AboutGitBook 安装 sudo npm install -g gitbook-cli 基本命令 gitbook init 初始化仓库, 可以根据SUMMARY.md的内容自行生成目录 gitbook build 可以不设置输出路径, 默认保存在_book目录下 gitbook install 根据book.json下载配置插件 gitbook配置 我的配置 同步coding和github coding page只支持 master和coding-pages的分支启动 coding page服务 github 默认支持 gh-pages 开启github page 服务 每次新建一个branch, 需要新建 .gitignore, 防止 _book 被更新到 git 中,这样 _book就成为一个与各个分支都不相关的目录, 不会git checkout的过程中丢失 各分支建立后, 同步脚本如下 gitbook build git checkout master git add . git commit -m $1 git push -u both master git checkout gh-pages cp -r _book/* . git add . git commit -m $1 git push -u origin gh-pages git checkout coding-pages cp -r _book/* . git add . git commit -m $1 git push -u coding coding-pages git checkout master Reference Publish GitBook to Your GitHub Pages By MagicWang，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2018-10-30 16:07:59 "}}